{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pValHMI9J4_E"
   },
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljNUHHy4dtrr",
    "outputId": "9fb18f70-4080-4fbe-8ec1-c2f5f5dc31b6"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install jiwer\n",
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331,
     "referenced_widgets": [
      "21bf28543f3b4691bd82e3fa730404bc",
      "565743e39af54104acfb3a65bd146d12",
      "ed29a057cbb9469eade1030af9613420",
      "d5f58cb1ac82459dae086bc882a21541",
      "5569b1567ef04090bd34345ddbdfb87c",
      "bc4a5ed7e8904d5e9909b1911a44fcd2",
      "fa06a3602d514bd1a5c1e3c57977bd0e",
      "25f6b44433bd48ab8362586783807e29",
      "9185dc530f2d4f36b6e61d597044dbc6",
      "ba13e624448f453d9f0d0a49991c8f48",
      "359c8f345ad940cc822f1612d0812917",
      "d6df85285c4c4c05ae7ce11e7fb7710b",
      "0baa97821a004dada03866cf3184c631",
      "f06f115fc5b34affae23553f68edd6b2",
      "093ee208c76d4935a4762d3908575449",
      "fe9ad4cf65c34b29ab2a5ad941d41090",
      "49627c0a4d4a4a6cb4fd58e21687e554"
     ]
    },
    "id": "A5myj4d-ZUPJ",
    "outputId": "0552b996-e25d-45bf-ca6a-ddeef39576f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8b264617c5443d94bd8a5505a66302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rvh0UUEDJ4_H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import librosa\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from mutagen.mp3 import MP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDNN Version : 8600\n",
      "Check CUDA/GPU Can Be Used : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"cuDNN Version : {torch.backends.cudnn.version()}\")\n",
    "print(f\"Check CUDA/GPU Can Be Used : {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1WTZdKUJ4_I"
   },
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_11_0 (/home/alckylzer/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/id/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b09958ad9049bba1a5ae95c215c132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 5048\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 3226\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 3618\n",
      "    })\n",
      "    other: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 24238\n",
      "    })\n",
      "    invalidated: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 2466\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "total_data = load_dataset('mozilla-foundation/common_voice_11_0', name='id')\n",
    "print(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "1b2ef971bb744c7a8da94f102bb68f92",
      "3778d11c7ce140d7899db623e2fa72c4",
      "72f85a062cf048a9ada39fece305c9af",
      "e6fd5fd2897e4004a0bf9e6c36c2ae83",
      "2159a3da41fa4fd287b16bea5ff5acbb",
      "5dec0278be58410291e0b9672af6d812",
      "f2462f13ba7b4977af91b2605445fd19",
      "fc09f2e26a8e4ce6a1f9b39d892330a1",
      "6aa25ba922074be09d8cde68ded9a3fa",
      "9f1a8f09ff85416daa847ee2025d20b1",
      "0e3d4a9e89704e49aee32e9b33d90f0a",
      "e9a74eb3a2ac4fe08c8766ccba462968",
      "f55c5eee4d7e4e51aa2a2af3ea0e0d91",
      "e3f5fb0b3645407a87b868bd7f26900a",
      "07d8358b58b749d78f712802d4642920",
      "52f21a90639841b0b8fa4e29eeee9967",
      "cd4e2b134ba1432badaba686204fb74f",
      "ec20405657d34dbbaa7052a0c0e61fae",
      "92a15f0b670b4378807ae319566ae225",
      "172998905a2446078a534772fe386cef",
      "4c1ae84d052a45668953ccb207e8622a",
      "395a2f38aa5b4796b64e13a28cc3f042",
      "5ce0eb3637ac4e0aaa876160263c3462",
      "5852fc7c3812481ba9013924ebabffc8",
      "52568063ae9a430cb24c22171e8a913c",
      "145c6a8522154ef6808e761f0a421db0",
      "7320e3b707094ae5981ae8740949acea",
      "793874e8471845d38031555e81ab58f7",
      "e8fb27b46a99462eb38bd6773855be99",
      "9cdf8e3fca46449a94015dd6c387dc98",
      "35dd0c070ad443fe8872bba4b1a8dcd3",
      "9abf078e8002478d9c45155a7e5c9a47",
      "82d38cd18c0240fcaf1e73f9a4304727",
      "0d2032b542e94073a0f704d358b24df0",
      "e14acb70286f477d96f03531706734e7",
      "f2d4528c6c4841b39c7418e9cb0cc2dd",
      "e88b8e0d39024ba5ade579f09d7a7673",
      "f614e335fcf5470dbf26a62d168d0298",
      "7e1a13e83ca84816ac8082d31db6cde5",
      "f67187555d4f498e89e57ff62af12973",
      "b312a20b8da94d5d9da7124fdfef4e15",
      "d48d95ec3cbe44978c982ea1905f901f",
      "810adcfa7d4f4a9584167354f6a5f256",
      "6244e4ab26974316a293c3a11735d534",
      "13cb6ac93068465f825241ef1cc4cc80",
      "d5f75016bb8840e496310f66b8c25952",
      "15bdba366b194bb2a6685111c6404e65",
      "b2bca451289a4f31ba1fdd9e582fe5f1",
      "df6a6811fc4447b9beba309f4191f548",
      "54e3ee36597147258034d3020f6c265f",
      "a161785a97b14ef791cce57bca1e7ffb",
      "600edab3d8104faea0edd551f5424486",
      "5bf445eb945a49d3b6021156129e309a",
      "ef7ad4e52ed74d4eb3d44a30cd3e4c93",
      "1b0eba2762ea4e489e0d6712f1a9ec53",
      "61878637885e4114b5d898674a721f1c",
      "cae78469154b44299047cd027e2390b1",
      "7c19bfee111d418dacf2c660e35412d5",
      "20cfb7779539410c835ec30a7235dec1",
      "f66e9ab747284a888484ee8f9e500f23",
      "530e93a3a8fc494e84e4fc805ec38706",
      "ca9857277b484ab1991779e0edac560d",
      "8d6c127e5adf4ab0bdff89b10ed8c07f",
      "43e9558d91744c3eb03ab2c0163f591d",
      "f1b33edad42b4a77bfcb9e59e9ea6082",
      "04e4d465d422467eb0e6d7ef596b8600",
      "41bb63225b4f45b997468ef34c639e14",
      "de11d75ec5954a408083b0dfe09f6365",
      "417c76fd05b545b08da3eb41ed6bf486",
      "ac43a4a1f00d4499a8a0bd4b3a42d8e5",
      "f60637a143984224947c2fde9999cfb0",
      "514ec2dff0bd4d179bfcfc31c7389fb6",
      "34e435f06fe24b37b44e851e6586657d",
      "b8beeedc05b44f59b2ae9098c818d980",
      "14eb9659ad4f4019a2f56c57c7e56ad6",
      "45598c606e8841cb8ed704e60c64bea3",
      "717ad4ec48ae4d24b6dd243262452c49",
      "cf3614f12ca6403780aab96f06245039",
      "8c3ce7ce0e614df18824605ff3026161",
      "36a3f04fb4824e48be53ee16e2d0cf82",
      "b047f0d63a83463f818f68f31b397942",
      "dcd1850834a94715af59c407d4edd87c",
      "0f34a9cb625740dcb28366a5cb9453ab",
      "aac920cef15148b389134446b551cc8d",
      "43671c30c23b48a5bdf763e07e03c62d",
      "5b31769330664de2a3b9d312bc8c5672",
      "52643394b36446b6b9f7ff84307fa05f",
      "da6d00f4634f468281369373bfd79dee",
      "6e3ce2be7ac14f9b89dd575cb55538ad",
      "5af7ef2ba1734cd9acdee90a26dcb24e",
      "374df3c3e3ff4cc9be6b4c62d54dcfd8",
      "d0ab98219e4342e6bf4df50f92948bfe",
      "9a19ec7125684a9692e2c587d6038b15",
      "6fd4a51bf9a5451d99d2e2fd28a343b2",
      "7d27dce0505143279b617b3441e0c803",
      "3c7983af3b94453897b1bfcc3947d8b3",
      "02ade06b773542b194eb7aee1e780a13",
      "6fa8de794f4243b9afe5d312c055feb3",
      "e6578f41f18044d39b11cf9a565dcf58",
      "6e725578b7194b9cb9a60c07f9c85b3c",
      "25b27a519e7c48c4ad6786147f20d584",
      "ad0ece3f2737452b95055d1fed082737",
      "3b49e73cdd2e4aa5a20ba982251c47f8",
      "8c9f7bcd5b1b40bd8ed570f6f59e7f69",
      "f8ccafec29aa4a55ae57f436edf7978a",
      "808e1809e00d4ce7a0db587249ee0d3e",
      "adffdd42aa0c4e2187e2ab01fd8712c9",
      "848ae6243e9a4574a2da7c9481dec9c5",
      "6b47d88cf716414e9ca82037be0ad773",
      "3810557bbdf2418abfdf580c1bc5ddb9"
     ]
    },
    "id": "tMQtrgOdJ4_I",
    "outputId": "b12413b8-15e3-4ef4-b9a7-bbf4bdfbfafb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_11_0 (/home/alckylzer/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/id/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n",
      "Found cached dataset common_voice_11_0 (/home/alckylzer/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/id/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n"
     ]
    }
   ],
   "source": [
    "# Main Dataset\n",
    "data_train = load_dataset('mozilla-foundation/common_voice_11_0', name='id', split=\"train+validation\", use_auth_token=True)\n",
    "data_test = load_dataset('mozilla-foundation/common_voice_11_0', name='id', split=\"test\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration id-language=id\n"
     ]
    }
   ],
   "source": [
    "# Corpus for LM Dataset\n",
    "oscar_corpus = load_dataset(\"oscar-corpus/OSCAR-2201\", use_auth_token=True, language=\"id\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fjDUNraJ4_K",
    "outputId": "07228170-9558-45c6-820d-eb3be6f528b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Train Features : \n",
      " {'client_id': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None), 'sentence': Value(dtype='string', id=None), 'up_votes': Value(dtype='int64', id=None), 'down_votes': Value(dtype='int64', id=None), 'age': Value(dtype='string', id=None), 'gender': Value(dtype='string', id=None), 'accent': Value(dtype='string', id=None), 'locale': Value(dtype='string', id=None), 'segment': Value(dtype='string', id=None)}\n",
      "\n",
      "\n",
      "Data Test Features : \n",
      " {'client_id': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None), 'sentence': Value(dtype='string', id=None), 'up_votes': Value(dtype='int64', id=None), 'down_votes': Value(dtype='int64', id=None), 'age': Value(dtype='string', id=None), 'gender': Value(dtype='string', id=None), 'accent': Value(dtype='string', id=None), 'locale': Value(dtype='string', id=None), 'segment': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Train Features : \\n {data_train.features}\\n\\n\")\n",
    "print(f\"Data Test Features : \\n {data_test.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Train : 8274 atau 70%\n",
      "Total Data Test : 3618 atau 30%\n"
     ]
    }
   ],
   "source": [
    "total_dataset = data_train.num_rows + data_test.num_rows\n",
    "data_train_percent =  data_train.num_rows / total_dataset * 100\n",
    "data_test_percent = data_test.num_rows / total_dataset * 100\n",
    "\n",
    "print(f\"Total Data Train : {data_train.num_rows} atau {round(data_train_percent)}%\")\n",
    "print(f\"Total Data Test : {data_test.num_rows} atau {round(data_test_percent)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjiiiFkvJ4_L"
   },
   "source": [
    "Remove Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HAjRm6VcJ4_L"
   },
   "outputs": [],
   "source": [
    "data_train = data_train.remove_columns(['client_id', 'audio', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'])\n",
    "data_test = data_test.remove_columns(['client_id',  'audio', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwR7WkquJ4_L",
    "outputId": "cff169e5-052e-4fa6-bbb1-8475aad4dd25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Train Features : \n",
      " {'path': Value(dtype='string', id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "\n",
      "\n",
      "Data Test Features : \n",
      " {'path': Value(dtype='string', id=None), 'sentence': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Train Features : \\n {data_train.features}\\n\\n\")\n",
    "print(f\"Data Test Features : \\n {data_test.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Train = 11 Jam : 33 Menit : 28 Detik\n",
      "Data Test = 4 Jam : 7 Menit : 27 Detik\n"
     ]
    }
   ],
   "source": [
    "data_train_path = list(data_train['path'])\n",
    "data_train_length = 0\n",
    "for path in data_train_path:\n",
    "    audio = MP3(path)\n",
    "    length = audio.info.length\n",
    "    data_train_length += length\n",
    "print(f\"Data Train = {int(data_train_length)//3600} Jam : {int(data_train_length)%3600//60} Menit : {int(data_train_length)%3600%60} Detik\")\n",
    "\n",
    "data_test_path = list(data_test['path'])\n",
    "data_test_length = 0\n",
    "for path in data_test_path:\n",
    "    audio = MP3(path)\n",
    "    length = audio.info.length\n",
    "    data_test_length += length\n",
    "print(f\"Data Test = {int(data_test_length)//3600} Jam : {int(data_test_length)%3600//60} Menit : {int(data_test_length)%3600%60} Detik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECFHkhLtJ4_M"
   },
   "source": [
    "Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Jq_-RynedtDa"
   },
   "outputs": [],
   "source": [
    "import audioread\n",
    "from librosa.util import buf_to_float\n",
    "\n",
    "def audioread_load(path, offset=0.0, duration=None, dtype=np.float32):\n",
    "    y = []\n",
    "    with audioread.audio_open(path) as input_file:\n",
    "        sr_native = input_file.samplerate\n",
    "        n_channels = input_file.channels\n",
    "\n",
    "        s_start = int(np.round(sr_native * offset)) * n_channels\n",
    "\n",
    "        if duration is None:\n",
    "            s_end = np.inf\n",
    "        else:\n",
    "            s_end = s_start + (int(np.round(sr_native * duration)) * n_channels)\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        for frame in input_file:\n",
    "            frame = buf_to_float(frame, dtype=dtype)\n",
    "            n_prev = n\n",
    "            n = n + len(frame)\n",
    "\n",
    "            if n < s_start:\n",
    "                # offset is after the current frame\n",
    "                # keep reading\n",
    "                continue\n",
    "\n",
    "            if s_end < n_prev:\n",
    "                # we're off the end.  stop reading\n",
    "                break\n",
    "\n",
    "            if s_end < n:\n",
    "                # the end is in this frame.  crop.\n",
    "                frame = frame[: s_end - n_prev]\n",
    "\n",
    "            if n_prev <= s_start <= n:\n",
    "                # beginning is in this frame\n",
    "                frame = frame[(s_start - n_prev) :]\n",
    "\n",
    "            # tack on the current frame\n",
    "            y.append(frame)\n",
    "\n",
    "    if y:\n",
    "        y = np.concatenate(y)\n",
    "        if n_channels > 1:\n",
    "            y = y.reshape((-1, n_channels)).T\n",
    "    else:\n",
    "        y = np.empty(0, dtype=dtype)\n",
    "\n",
    "    return y, sr_native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "42cc4c447a2b4309bcc47165d8311524",
      "627a56521edc4e3d8994d85f75bfb6dc",
      "dcc4e418a81b44b8b6db59f48f8b8546",
      "aab3d670f4054b9096f30a0ddd49c55f",
      "d0407fe7e3e1494d828c7fe5311c2b85",
      "22f9610cc6ca425788d5f603c970842b",
      "2591c464ba2e410fb557d1cc314e5c36",
      "47effa143ac0447e93b2b157bbc75e36",
      "c7c9304d67154faf8b0e6ec111bf4ba6",
      "5a8a886f891d4b208f749a824c6db0d7",
      "ed7effe2c6444383a8b640d8b283cd16",
      "8474fc0b65444840bd5733c7cf4669ac",
      "9b5099e8eb444c60b9f3fea2c27ad790",
      "a2b2c9415167433882d4460e4212e7a3",
      "b662c487752e42e7911b3c597e82dd10",
      "14d940f186c44d5289c3d02c0f1105a3",
      "1d1c7b11ecd4405fa40e2479d024b6b5",
      "6499517033d34158a0d793a01e816bf5",
      "14f69f441b8444339867a1cacd63840a",
      "dc74e714545d4b34b5fe1da7380074e0",
      "e5585d762b4f4e1396e1b14c748f1e95",
      "50d3ed6847e945ea9833e1485f1fc796"
     ]
    },
    "id": "LIOcJYsjJ4_M",
    "outputId": "a990f736-7baa-40ca-edd2-56f3d2d9918d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alckylzer/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/id/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0/cache-8df27d66ddf84352.arrow\n",
      "Loading cached processed dataset at /home/alckylzer/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/id/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0/cache-32dbb2877c311299.arrow\n"
     ]
    }
   ],
   "source": [
    "def resample(batch):\n",
    "    audio, sample_rate = audioread_load(batch['path'])\n",
    "    audio = librosa.to_mono(audio)\n",
    "    batch['audio_resampled'] = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000) \n",
    "\n",
    "    return batch\n",
    "    \n",
    "data_train_resample = data_train.map(resample)\n",
    "data_test_resample = data_test.map(resample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_resample['audio_resampled'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWjFgdiPJ4_M",
    "outputId": "de1af965-5ef5-4a30-c27a-6004f2f5c147"
   },
   "outputs": [],
   "source": [
    "print(f\"Data Train Features : \\n {data_train_resample.features}\\n\\n\")\n",
    "print(f\"Data Test Features : \\n {data_test_resample.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKTq23q9J4_N"
   },
   "source": [
    "Clean Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Train Sentence Example : \\n\")\n",
    "for i in range(10):\n",
    "    print(data_train_resample['sentence'][i])\n",
    "\n",
    "print(\"\\n\\nData Test Sentence Example : \\n\")\n",
    "for i in range(10):\n",
    "    print(data_test_resample['sentence'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtXyUvDlJ4_N"
   },
   "outputs": [],
   "source": [
    "def cleaning_sentence(dataframe, oscar = False):\n",
    "    chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\'\\[\\]\\(\\)\\~\\|\\\\\\]'\n",
    "    if oscar == False:\n",
    "      col = 'sentence'\n",
    "    else:\n",
    "      col = 'text'\n",
    "\n",
    "    dataframe[col] = dataframe[col].encode(\"ascii\", \"ignore\").decode()\n",
    "    dataframe[col] = dataframe[col].lower()\n",
    "    dataframe[col] = re.sub(chars_to_remove_regex, '', dataframe[col])\n",
    "    dataframe[col] = re.sub('&', 'dan', dataframe[col])\n",
    "    dataframe[col] = re.sub('#', 'hashtag', dataframe[col])\n",
    "    dataframe[col] = re.sub('@', 'at', dataframe[col])\n",
    "    dataframe[col] = ' '.join(dataframe[col].split())\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "5937076313cb42318198969dd95fb2c1",
      "22cfa4aa0d344ef58750d682201d0df4",
      "3a7d2dcd4fb54b4c924824786793d4b4",
      "a49eef580a6b443db316669001dbef8a",
      "26aef4cb3aae490594d08741bd1c7f01",
      "cd5d54a1bea04710b4eec7eb2c3db64b",
      "9147269725ed458485957cf3cb59de6b",
      "9c2bfa73d1fa4aaebf81a36b1c7d3f50",
      "cced39533ca549e383feda610fd62a06",
      "adc8513f407248e5a43a096925186f36",
      "1689928c53834886ad9d9fffdbad790b",
      "f6c276dd782c4fbc8b00ec2ea84bcb5c",
      "c48c408e748a4bb08ea5bee9ffa0ecc9",
      "47775064ab05427e9e7a8880be22d374",
      "5a82c412c6584a67888319b31d292398",
      "44e938a335bc4e11868d4231d7abbafc",
      "ad3ab52c598c44db98b63bbe512b0a24",
      "a69451deab4c432e98233e38450a1152",
      "c41061f6ff684efd8553e8398fbe0921",
      "2f0539d7226749c4a8bffab79cdc7aed",
      "462abd12eaf948f4a1435f65307a0d6a",
      "e35062f99bd14e6280569d3d47113bfd"
     ]
    },
    "id": "YXEj4oHgJ4_W",
    "outputId": "1542bbe6-963b-468c-d651-91f5b2949913"
   },
   "outputs": [],
   "source": [
    "data_train_cleaned = data_train_resample.map(cleaning_sentence)\n",
    "data_test_cleaned = data_test_resample.map(cleaning_sentence)\n",
    "oscar_corpus_cleaned = oscar_corpus.map(lambda x: cleaning_sentence(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = list(list(data_train_cleaned['sentence']) + list(data_test_cleaned['sentence']))\n",
    "\n",
    "with open(\"text.txt\", \"w\") as file:\n",
    "  file.write(\" \".join(text))\n",
    "\n",
    "for data in oscar_corpus_cleaned.take(1_000_000):\n",
    "    with open(\"text.txt\", \"a\") as file:\n",
    "      file.write(\" \".join(data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Train Sentence Cleaned Example : \n",
      "\n",
      "halo dunia\n",
      "sudah makan sudah sholat\n",
      "udah keluar hasil testnya\n",
      "dimanakah sate paling enak di jakarta selatan\n",
      "coba terus sampai berhasil\n",
      "kapan saya harus melapor pajak\n",
      "biro terdiri dari ketua komite dua wakil ketua dan pelapor\n",
      "seorang anak lakilaki berkulit hitam mengenakan celana jins biru dan baju berwarna terang sedang menatap tangannya\n",
      "dia mengakhiri musim bermain untuk wellington di liga birmingham\n",
      "setiap dewan memiliki ketua yang dipilih oleh dewan\n",
      "\n",
      "\n",
      "Data Test Sentence Cleaned Example : \n",
      "\n",
      "maha suci allah\n",
      "inilah dunia kecil\n",
      "nol\n",
      "aku tidak tahu artinya kebencian\n",
      "tugas saya belom kelar\n",
      "itu tadi menarik ya\n",
      "saya paling suka lagu ini\n",
      "jalanan sepi dari kendaraan\n",
      "jangan pergi\n",
      "semua kamus mengandung kesalahan\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Train Sentence Cleaned Example : \\n\")\n",
    "for i in range(10):\n",
    "    print(data_train_cleaned['sentence'][i])\n",
    "\n",
    "print(\"\\n\\nData Test Sentence Cleaned Example : \\n\")\n",
    "for i in range(10):\n",
    "    print(data_test_cleaned['sentence'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IDuhArJ4_W"
   },
   "source": [
    "Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0UbPvleUJ4_X"
   },
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for w in data_train_cleaned['sentence']:\n",
    "    vocab_list.extend(\" \".join(w))\n",
    "for w in data_test_cleaned['sentence']:\n",
    "    vocab_list.extend(\" \".join(w))\n",
    "vocab_list = list(set(vocab_list))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nbg99-qJ4_X",
    "outputId": "b1d2ef2a-0941-44a6-fdf2-ca9a6db5acf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary List :\n",
      " {'j': 0, 'k': 1, 'e': 2, 'h': 3, 'd': 4, 's': 5, 'f': 6, 'n': 7, 'y': 8, 'i': 9, 'p': 10, 'g': 11, 'o': 12, 'b': 13, 'c': 14, 'v': 15, 'x': 16, 'z': 17, 'r': 18, 't': 19, 'l': 20, 'm': 21, 'q': 23, 'u': 24, 'a': 25, 'w': 26, '|': 22, '[UNK]': 27, '[PAD]': 28}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary List :\\n {vocab_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk1jHA57J4_Y"
   },
   "source": [
    "Save Vocab (To .json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "-jiCtWC3J4_Y"
   },
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C17JdLC9J4_Y"
   },
   "source": [
    "Tokenizer (Encode & Tokenize Every Letter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Bag7K6AjJ4_Z"
   },
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\"vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBPmC2D_aKUz",
    "outputId": "c3ea1eae-9600-448e-a70a-89daf66c0ba1"
   },
   "outputs": [],
   "source": [
    "# tokenizer.push_to_hub(\"asr_skripsi_local_common_voice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnM-8TswJ4_Z"
   },
   "source": [
    "Feature Extractor (Return Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "2mNRjalkJ4_Z"
   },
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16_000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dzFMEQZJ4_a"
   },
   "source": [
    "Processor (Combine Tokenizer & Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "4Kcay6oBJ4_a"
   },
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor.save_pretrained(\"asr_skripsi_local_common_voice/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Languange Modelling n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/alckylzer/Desktop/ASR/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 5364062058 types 15518\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:186216 2:1138852352 3:2135348224 4:3416556800 5:4982479360\n",
      "Statistics:\n",
      "1 15517 D1=0.675778 D2=1.0492 D3+=1.2184\n",
      "2 99862 D1=0.773385 D2=0.883375 D3+=1.11667\n",
      "3 368178 D1=0.713328 D2=0.880577 D3+=1.1837\n",
      "4 1851617 D1=0.642623 D2=0.962668 D3+=1.30063\n",
      "5 8085148 D1=0.598595 D2=1.02009 D3+=1.39688\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 192 assuming -p 1.5\n",
      "probing 205 assuming -r models -p 1.5\n",
      "trie     71 without quantization\n",
      "trie     36 assuming -q 8 -b 8 quantization \n",
      "trie     67 assuming -a 22 array pointer compression\n",
      "trie     32 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:186204 2:1597792 3:7363560 4:44438808 5:226384144\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:186204 2:1597792 3:7363560 4:44438808 5:226384144\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:11571468 kB\tVmRSS:14688 kB\tRSSMax:2254988 kB\tuser:768.412\tsys:9.76961\tCPU:778.182\treal:790.11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "lm_text_path = cwd + \"/text.txt\"\n",
    "kenlm_path = \"kenlm/build/bin/\"\n",
    "os.system(f\"{kenlm_path}lmplz -o 5 <'{lm_text_path}' > '5gram.arpa'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=15517\n",
      "ngram 2=99862\n",
      "ngram 3=368178\n",
      "ngram 4=1851617\n",
      "ngram 5=8085148\n",
      "\n",
      "\\1-grams:\n",
      "-5.0476174\t<unk>\t0\n",
      "0\t<s>\t-0.11160436\n",
      "-4.1142364\thalo\t-0.11160436\n",
      "-3.541874\tdunia\t-0.15686736\n",
      "-3.023158\tsudah\t-0.23925924\n",
      "-3.09807\tmakan\t-0.3598049\n",
      "-4.913347\tsholat\t-0.11160436\n",
      "-4.4339643\tudah\t-0.11160436\n",
      "-3.435012\tkeluar\t-0.32991758\n",
      "-3.683923\thasil\t-0.16463552\n",
      "-4.7332344\ttestnya\t-0.11160436\n",
      "-4.4339643\tdimanakah\t-0.11160436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"head -20 {cwd}/5gram.arpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=15518\n",
      "ngram 2=99862\n",
      "ngram 3=368178\n",
      "ngram 4=1851617\n",
      "ngram 5=8085148\n",
      "\n",
      "\\1-grams:\n",
      "-5.0476174\t<unk>\t0\n",
      "0\t<s>\t-0.11160436\n",
      "0\t</s>\t-0.11160436\n",
      "-4.1142364\thalo\t-0.11160436\n",
      "-3.541874\tdunia\t-0.15686736\n",
      "-3.023158\tsudah\t-0.23925924\n",
      "-3.09807\tmakan\t-0.3598049\n",
      "-4.913347\tsholat\t-0.11160436\n",
      "-4.4339643\tudah\t-0.11160436\n",
      "-3.435012\tkeluar\t-0.32991758\n",
      "-3.683923\thasil\t-0.16463552\n",
      "-4.7332344\ttestnya\t-0.11160436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"head -20 {cwd}/5gram_correct.arpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j', 'k', 'e', 'h', 'd', 's', 'f', 'n', 'y', 'i', 'p', 'g', 'o', 'b', 'c', 'v', 'x', 'z', 'r', 't', 'l', 'm', ' ', 'q', 'u', 'a', 'w', '[UNK]', '[PAD]', '<s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=\"5gram_correct.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'j': 0,\n",
       " 'k': 1,\n",
       " 'e': 2,\n",
       " 'h': 3,\n",
       " 'd': 4,\n",
       " 's': 5,\n",
       " 'f': 6,\n",
       " 'n': 7,\n",
       " 'y': 8,\n",
       " 'i': 9,\n",
       " 'p': 10,\n",
       " 'g': 11,\n",
       " 'o': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'v': 15,\n",
       " 'x': 16,\n",
       " 'z': 17,\n",
       " 'r': 18,\n",
       " 't': 19,\n",
       " 'l': 20,\n",
       " 'm': 21,\n",
       " '|': 22,\n",
       " 'q': 23,\n",
       " 'u': 24,\n",
       " 'a': 25,\n",
       " 'w': 26,\n",
       " '[unk]': 27,\n",
       " '[pad]': 28}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")\n",
    "if os.path.exists(f\"{cwd}/asr_LM_skripsi_local_common_voice/\"):\n",
    "    os.system(f\"rm -rf {cwd}/asr_LM_skripsi_local_common_voice/\")\n",
    "    processor_with_lm.save_pretrained(\"asr_LM_skripsi_local_common_voice/\")\n",
    "                  \n",
    "else:\n",
    "    processor_with_lm.save_pretrained(\"asr_LM_skripsi_local_common_voice/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0K]  \u001b[01;34masr_LM_skripsi_local_common_voice/\u001b[0m\n",
      "├── [ 178]  alphabet.json\n",
      "├── [4.0K]  \u001b[01;34mlanguage_model\u001b[0m\n",
      "│   ├── [238M]  5gram_correct.arpa\n",
      "│   ├── [  78]  attrs.json\n",
      "│   └── [120K]  unigrams.txt\n",
      "├── [ 262]  preprocessor_config.json\n",
      "├── [  96]  special_tokens_map.json\n",
      "├── [ 339]  tokenizer_config.json\n",
      "└── [ 320]  vocab.json\n",
      "\n",
      "1 directory, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree -h asr_LM_skripsi_local_common_voice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading asr_LM_skripsi_local_common_voice/language_model/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"{kenlm_path}build_binary asr_LM_skripsi_local_common_voice/language_model/5gram_correct.arpa asr_LM_skripsi_local_common_voice/language_model/5gram.bin\")\n",
    "os.system(\"rm asr_LM_skripsi_local_common_voice/language_model/5gram_correct.arpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81hAn2aSfWpR"
   },
   "source": [
    "Resample Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "oIV_zTufJ4_b"
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print(f\"Sentence : {data_train_cleaned['sentence'][i]}\")\n",
    "#     ipd.Audio(data=data_train_cleaned['audio_resampled'][i], autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrF9SxsqfbcM"
   },
   "source": [
    "Preprocesss Input with Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "CHQkXCuAJ4_b"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"audio_resampled\"], sampling_rate=16_000).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "6f383279336a41a5ba8c6df02d4a689d",
      "3c85de102b43498f9117c06a798c2994",
      "8085aa670f8c45b9a9671456fd6a28c8",
      "8162aac48e8e4f5dbeee08f403beb235",
      "a61775339c574762a17ede1c29f280cd",
      "a3d0733688914d1ab6fd23ef43ca01fb",
      "a1450e2face8437f8280de3b67775edb",
      "d0aad81ceede45f0a8df5dec27895dc4",
      "2c7ae9252d2946d3ad62c58085809ada",
      "691c41b7ac8546f583070e6c02def73c",
      "e6c8a57d136f4884b7ac60698c3eb2e8",
      "f9048dffeac64bfeb3964c9e27c7dabe",
      "9093a084d1a2427fae93dbb3c57eb4fb",
      "534595cd5016456189ca9ba342d56df5",
      "cbd6772def4e49a4983d0a9a169731c0",
      "ac131c1adef54e40b2d4aba410a29d6e",
      "e0a036fd7d3c4452b089b186569d3ae1",
      "5e486a606ceb433eaf55584d4ff22548",
      "5f34163919854798b2738f49eac67b09",
      "704c23e0573545fe942e1c28ffba755a",
      "22e09fcca62d4bce93537273cc9d06a5",
      "69061081d9fd4840986ca17026492400"
     ]
    },
    "id": "HEI_G5vVdtDf",
    "outputId": "820bdb3f-bb03-4fdc-8b05-f7447b5cbdd1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69b4fe15d2f46cba9688ccaed955992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8274 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b92c96552c146d3bf09276cf8d4f2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3618 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train_input = data_train_cleaned.map(prepare_dataset, remove_columns=data_train_cleaned.column_names)\n",
    "data_test_input = data_test_cleaned.map(prepare_dataset, remove_columns=data_test_cleaned.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJCxnHzQfhji"
   },
   "source": [
    "Remove Audio That Has More Than 5sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "ef19bbea73ba45f38142c5cfebb31ce6",
      "c1172f488d1a421eb7cb7e325d60fd37",
      "6057e9a5e2d44012abe338f9ccdf1d11",
      "0adca7e2489f4abcad4c9417133dca68",
      "a2d027b4d7c340c89044b66b3585b41c",
      "d020799af2f8410d982c4286534aa59f",
      "4c3ceab40c1c483e87b1adf888fd6ab9",
      "c13aca97110e4c8a8fb213705f5ec6e0",
      "ca794c3f519d4d2da7dbd259d5bc599c",
      "2fff25c6afa647ce92e0c157f809d93d",
      "902a9a8dc780439c8218a91a2feaaade",
      "e49b36f20af148f68d7d4fe87816c3ee",
      "40107b46862e499586b3c86f49c545fb",
      "4b11dd76c1654b9cab40b765347ffc69",
      "896d4e8b262a4bcc96875979bc5605df",
      "40fabc4b212f4be2b152c009a2b8c162",
      "014dff913f674b37a187406fa6772ee6",
      "cbcdce31fb224f7eae62ab5c50146b18",
      "9a9ed35374fb4dd881565efe7b809275",
      "c01167c22ea84250853a9f50f34f1242",
      "17a6f25efb724f6e97df5f52fa0e237f",
      "872fedd7acdf497e8779f53510bceafb"
     ]
    },
    "id": "om1My8ElJ4_b",
    "outputId": "6cafb80a-3b73-4f9d-d4b9-39cd9b58132b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1aba38d10564996b834830693bd9e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ff3c6dc3fa45c195323e84d56a286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length_in_sec = 5.0\n",
    "data_train_filtered = data_train_input.filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])\n",
    "data_test_filtered = data_test_input.filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAbhGodxfznG"
   },
   "source": [
    "Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mQSmEKFjJ4_c"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "E18TBGDZJ4_d"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73Lq8VrUf9cL"
   },
   "source": [
    "Word Error Rate Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "9c4ed56212654bd2bc7bf8bef9cccc06",
      "0554bea1bfc84f85a72804c3e2258924",
      "c23451c08fa34392a0a5dd981ceb048c",
      "8ac9e76d6d7c4d07b3872a384ca67dd9",
      "8d757e9f95ea416c910327fec4e571c2",
      "66be9c45c4ed465fb318ecd7285a50b8",
      "2ed25f60920f4e769abbb4144004dd1a",
      "2468be5200fc41df84c7b2220ac22d9a",
      "bb9f55af3ac04173bde3b0580bdeb7ea",
      "14f8cb8989244387a2e8a34102c34bf5",
      "021e5c1e54724325bb42a86fc315bdb3"
     ]
    },
    "id": "jbExE6WfJ4_d",
    "outputId": "cecd8a17-b9c1-4a01-f6db-cd48b730886b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5003/24688115.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seI5290bgBRE"
   },
   "source": [
    "Computer WER While Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "fCs7AhfIJ4_d"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6DsEipFgG2A"
   },
   "source": [
    "Clear Cache and Trainer Model Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del trainer\n",
    "# del model\n",
    "# del training_args\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0160, -0.0121, -0.0101,  0.0007, -0.0418, -0.0089, -0.0063, -0.0200,\n",
       "        -0.0106, -0.0140, -0.0244, -0.0264, -0.0341, -0.0121, -0.0232, -0.0013,\n",
       "        -0.0211, -0.0273, -0.0090, -0.0077,  0.0071, -0.0165, -0.0511, -0.0129,\n",
       "        -0.0185, -0.0085, -0.1896,  0.0354])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(f\"/home/alckylzer/.cache/huggingface/hub/models--indonesian-nlp--wav2vec2-large-xlsr-indonesian/snapshots/68fbcbd947e32184a704b401b71973d6c27de0c1/pytorch_model.bin\", map_location='cpu')\n",
    "state_dict.pop('lm_head.weight')\n",
    "state_dict.pop('lm_head.bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "d8babcdad96747b9a8ad573b730fc2ac",
      "59dedebc2f5f4b0480626d33f9b793a6",
      "b8a49e39a094476c8b57f949a313fded",
      "32f4a6fe5bc44dfc9bca3b85ca1ee5b2",
      "b5d78ea64cdc42b591c73e19ada2c5fa",
      "0a5bd587437643ef9e7ebd971623fb92",
      "491331f710614484ad3af017ac884075",
      "6a2b9a36a64546b498065deffd31183d",
      "b531ed66a483406a802ba9a463e179a1",
      "10734c5a0c9047e6ab6b77eb8ed9a83c",
      "be793e2bef754139b59d1120a255b37e",
      "205cc51fdbb54474b74985c828c70cea",
      "867377788bf149c3b0b9bfbd6344e9c9",
      "cf275e0f0a444b8d8eef959700d1ca4d",
      "bbb11666da554de292984bedfcf830f5",
      "42dfdfa424ea4ee4b2ee37bbeadeef7b",
      "dcbd9dcd0b5b4b4394b16f59671eb5f5",
      "4f6633174fea4e7aafc5e0e577aadfd6",
      "0281f82409a34029b425b453269a02bf",
      "142be833a9864a0fbdddc4944ee48321",
      "b4339db9bc234ab19d06ac49d3a17fb2",
      "a3d02dd0a8714acdb562276b69288181"
     ]
    },
    "id": "xVsSv8CPJ4_d",
    "outputId": "2a24c1f8-db04-4060-de06-5b22af107a8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at indonesian-nlp/wav2vec2-large-xlsr-indonesian and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"indonesian-nlp/wav2vec2-large-xlsr-indonesian\",\n",
    "    #############################\n",
    "    # attention_dropout=0.094,\n",
    "    # hidden_dropout=0.047,\n",
    "    # feat_proj_dropout=0.04,\n",
    "    # mask_time_prob=0.04,\n",
    "    # layerdrop=0.041,\n",
    "    # activation_dropout=0.055,\n",
    "    #############################\n",
    "    # attention_dropout=0.094,\n",
    "    # hidden_dropout=0.047,\n",
    "    # feat_proj_dropout=0.04,\n",
    "    # mask_time_prob=0.082,\n",
    "    # layerdrop=0.041,\n",
    "    # activation_dropout=0.055,\n",
    "    #############################\n",
    "    # attention_dropout=0.3,\n",
    "    # activation_dropout=0.2,\n",
    "    # hidden_dropout=0.3,\n",
    "    # mask_time_prob=0.05,\n",
    "    #############################\n",
    "    # attention_dropout=0.0,\n",
    "    # hidden_dropout=0.0,\n",
    "    # feat_proj_dropout=0.0,\n",
    "    # mask_time_prob=0.05,\n",
    "    # layerdrop=0.0,\n",
    "    state_dict=state_dict,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    bos_token_id=processor.tokenizer.bos_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vuIModggMBy"
   },
   "source": [
    "Freeze Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "CTCkq5L4J4_e"
   },
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "1b-tca5PJ4_e"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    dataloader_num_workers=2,\n",
    "    output_dir=\"asr_skripsi_local_common_voice\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=20,\n",
    "    gradient_checkpointing=True,\n",
    "    save_steps=400,\n",
    "    warmup_steps=500,\n",
    "    eval_steps=400,\n",
    "    logging_steps=400,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    optim='adamw_bnb_8bit',\n",
    "    ###############################\n",
    "    learning_rate=7.5e-5,\n",
    "    # learning_rate=4.42184e-05,\n",
    "    # learning_rate=1e-4,\n",
    "    # weight_decay=0.0354792,\n",
    "    # weight_decay=1e-2,\n",
    "    # learning_rate=1e-4,\n",
    "    weight_decay=0.0005,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bitsandbytes as bnb\n",
    "# from transformers.trainer_pt_utils import get_parameter_names\n",
    "# from torch import nn\n",
    "\n",
    "# decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "# decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {\n",
    "#         \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "#         \"weight_decay\": training_args.weight_decay,\n",
    "#     },\n",
    "#     {\n",
    "#         \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "#         \"weight_decay\": 0.0,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# adam_bnb_optim = bnb.optim.Adam8bit(\n",
    "#     optimizer_grouped_parameters,\n",
    "#     betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "#     eps=training_args.adam_epsilon,\n",
    "#     lr=training_args.learning_rate,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325,
     "referenced_widgets": [
      "fc9443e681144110af0a067b4388927e",
      "1cf957eb9d9c409cbd86f0bd6bf2fe1d",
      "d3ce96725a654909aaed6da21283996b",
      "e260b41b249e486b841478347685c469",
      "6fa557c433734f9d8a52d8ac3ad3ce5f",
      "fcf7a224984e4f8196555cd956fadf52",
      "074c5579eb314638815d062aaae29e7c",
      "dc25371e9f9243f1a931081248b80f16",
      "466700526b0840f781aa8e77908a169e",
      "1615f6e8e2d1449e8763e3662e22fcc4",
      "7733dc5fc13d441f884e16dc3689a977",
      "ecd41cc714dd497580b13a33f560ca17",
      "db4fdba93caa4cfbb5b273dda8f33335",
      "4dfc6f9ea1224037907ba543fc714aaf",
      "c4023d888cb5473098425658d0a9c444",
      "e093bdf7f2d84a7bad87df22c37f8df1",
      "e185d8aa960d4bcda35a294387e726fd",
      "c8c3db53989d49be92d9732abca98e8c",
      "8351abbbdf01475fb68798070593ab30",
      "42c09691b1b74259b75e27feda1f0509",
      "77e09a25d6f74724b4c68387c4b1abcc",
      "3e7e8ae537a44122b81c4f71f900c1c3",
      "d7436465697b461c83d4dba9f0e4d0d0",
      "dc2cff10e21e4633a892f99699bd3336",
      "ebac0c645c1d4899bf08ccca6a3dfd23",
      "b624c57b4876458485509dd7228d92e1",
      "889a4c1459474d02b8480074cf060c26",
      "302cee7947294db99a255b93abdfc194",
      "c479c6e170ac4831a3c1752b2a4d377e",
      "770fac43acbb4c558fd20db3894629a1",
      "3a0bcf97279e47c38eb268dba0a2da2d",
      "4e5c8695e31b428e9b68e282e4c2cf03",
      "e19f96c114e948cb9c46570f0e53f760",
      "5271f25733d84fe1ae22146fcfeb8714",
      "98e81add4ecb4fbb8ed4349fae92e79a",
      "8368082eba014b5fb9c549b89c169193",
      "53cb4eb1d7674cfea588d840b9895747",
      "6c2e0053a957495c9724c23f327e0363",
      "e60147424ae84670ac733ccce60f3e8d",
      "650345c6011144aeb9d7303a777d1461",
      "bf846d317dd94e71b24ac86a190971ed",
      "2c4b8aa4b0004087895391b2a5b20a73",
      "75c12df1a297496b9c6a3b2b6910e5e9",
      "7bd6c2c692b74d31941db9e4363a6310",
      "4df18e9d1b204cb3b8391cf0526da2b4",
      "0864f1933835444ebdc577d7c43bbe31",
      "65158dff026248a4abb1d2214fb07c08",
      "e774cbcc2bbb4d6d8f6a71186e7a68be",
      "fee9b4d2ff2a4495991339a923726c3f",
      "df8606e75f0248dfbc5fb4219d2a9369",
      "f103b07e68dd4718a5b5950f61c250a5",
      "2cd9cdb11486408b9e1a4944b37935f6",
      "95b08b73a97a42b680aea0cdb53e212d",
      "f5353e7d489242bd9c27f06b041393f7",
      "408096a1dc6b420eb9ae26c288ac2cb9",
      "b00bf02e86364710be5996dc609bfe97",
      "e220dfc6a1a74891aefaf163cd36408e",
      "790b7e0a322c4567ba803700a9550fc3",
      "c572e31646ca4e38842acf76d91da525",
      "e034efcf050f4a4a8e16ba590e778080",
      "a60c7eaef53e4c719e1531ebb9fbb1f0",
      "a3605fcce106468281c90d52c9436635",
      "c8333af74f544498b77f773a2f54483e",
      "6f75edbcd0a44c4d8138bb124524bdaa",
      "5b2997be0de44def8882c496c7087a68",
      "a9c5403ce01a45d6a514631b1676b06a",
      "a6bb7c08dd6c4aec826b05e0d807f61b",
      "7c2b8071d2934c9ca163740b12606692",
      "e4481844a13b4c6ca7e0589e3c8cc864",
      "cbbb1515c45e4288b71091474bafe1c9",
      "725ff5d63bc94d8793f610e5e3f5ac94",
      "37872dfe95524d36804de2ba543c3a11",
      "91e2f27286444346980fe267041798dc",
      "08e723d02f1f4e30b4d685eecfbdcaf7",
      "edb74d5b24414a59baeed7309f0a9cf8",
      "1f8b53e7b66c4303a13723419e7e97a9",
      "c1630ba652e6449a879262d9e16e9b1e",
      "424a81df142c41869429c4ce13d40769",
      "ff91c83e97c841c8a114f7df669bb1bc",
      "845ee52b8b4a4ed9bd89f4d17b2ec89c",
      "1acb8de963824a67a040d32b88d69978",
      "07fd749cf7364a64b164fb802b2a6916",
      "7a7ada3e73b7418d8f4784b9c9444d30",
      "84a06fdc6c9d4586b7cdb907cf9423ef",
      "d3599af2f884471798a2edab262ba312",
      "df707b78089b4432b9b94f68b0a2125e",
      "3228b38b574d48059ae91bf748438f2b",
      "7638e96b8519408bbed9ada0f6f89275"
     ]
    },
    "id": "vhOPb-uXJ4_f",
    "outputId": "145262d7-9a66-4cb1-a18b-8818c6c6790d"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    # optimizers=(adam_bnb_optim, None),\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data_train_filtered,\n",
    "    eval_dataset=data_test_filtered,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QoD6vG_7J4_f",
    "outputId": "2f8defb9-2984-420f-eb74-b79fa12ab7c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4384\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2740\n",
      "  Number of trainable parameters = 311258269\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2740' max='2740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2740/2740 4:48:45, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.689600</td>\n",
       "      <td>0.314434</td>\n",
       "      <td>0.306355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.151268</td>\n",
       "      <td>0.272998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.151505</td>\n",
       "      <td>0.274743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>0.144252</td>\n",
       "      <td>0.272287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.404500</td>\n",
       "      <td>0.142727</td>\n",
       "      <td>0.268278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.143434</td>\n",
       "      <td>0.268214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2917\n",
      "  Batch size = 8\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to asr_skripsi_local_common_voice/checkpoint-400\n",
      "Configuration saved in asr_skripsi_local_common_voice/checkpoint-400/config.json\n",
      "Model weights saved in asr_skripsi_local_common_voice/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in asr_skripsi_local_common_voice/checkpoint-400/preprocessor_config.json\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2917\n",
      "  Batch size = 8\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to asr_skripsi_local_common_voice/checkpoint-800\n",
      "Configuration saved in asr_skripsi_local_common_voice/checkpoint-800/config.json\n",
      "Model weights saved in asr_skripsi_local_common_voice/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in asr_skripsi_local_common_voice/checkpoint-800/preprocessor_config.json\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2917\n",
      "  Batch size = 8\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to asr_skripsi_local_common_voice/checkpoint-1200\n",
      "Configuration saved in asr_skripsi_local_common_voice/checkpoint-1200/config.json\n",
      "Model weights saved in asr_skripsi_local_common_voice/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in asr_skripsi_local_common_voice/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [asr_skripsi_local_common_voice/checkpoint-400] due to args.save_total_limit\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2917\n",
      "  Batch size = 8\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to asr_skripsi_local_common_voice/checkpoint-1600\n",
      "Configuration saved in asr_skripsi_local_common_voice/checkpoint-1600/config.json\n",
      "Model weights saved in asr_skripsi_local_common_voice/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in asr_skripsi_local_common_voice/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [asr_skripsi_local_common_voice/checkpoint-800] due to args.save_total_limit\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2917\n",
      "  Batch size = 8\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to asr_skripsi_local_common_voice/checkpoint-2000\n",
      "Configuration saved in asr_skripsi_local_common_voice/checkpoint-2000/config.json\n",
      "Model weights saved in asr_skripsi_local_common_voice/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in asr_skripsi_local_common_voice/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [asr_skripsi_local_common_voice/checkpoint-1200] due to args.save_total_limit\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2917\n",
      "  Batch size = 8\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to asr_skripsi_local_common_voice/checkpoint-2400\n",
      "Configuration saved in asr_skripsi_local_common_voice/checkpoint-2400/config.json\n",
      "Model weights saved in asr_skripsi_local_common_voice/checkpoint-2400/pytorch_model.bin\n",
      "Feature extractor saved in asr_skripsi_local_common_voice/checkpoint-2400/preprocessor_config.json\n",
      "Deleting older checkpoint [asr_skripsi_local_common_voice/checkpoint-1600] due to args.save_total_limit\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2740, training_loss=1.062227254714409, metrics={'train_runtime': 17336.4129, 'train_samples_per_second': 5.058, 'train_steps_per_second': 0.158, 'total_flos': 1.0045163306413486e+19, 'train_loss': 1.062227254714409, 'epoch': 20.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516,
     "referenced_widgets": [
      "5d785eb7883d4911bdad17ad432ebcb2",
      "ce7f42b501fa421c81b80aa1e0d22dd0",
      "fec7e0ed8d2e4aa0a9c5ab4a1defd80b",
      "b51935d7c2454262860939d5fa90c7df",
      "ed079c489ae64a7b9876ad66565b48c5",
      "bc4be33a02b54cc59343ba73c33fab78",
      "00e276ccba3f409c9aa7b1c358288da6",
      "45d6fe9e2e6e4c82a17aab8cc85c6386",
      "6fc1bba0966847ab9f3cdbbd01d3ca1d",
      "202019cc1b8c423bba3bed5e7b0bea08",
      "9b41b84cfdf5473b82c58aab4eadbe3d",
      "77783d22bd8e423abe5ef98d544de03a",
      "7eed8a27e9194a8b981e8dfd5b856e70",
      "c12eb19f47484060a957104880638fef",
      "cabea139288b434396e446fcf35d9b5b",
      "753e8a5f3b97405587743d8b23aac906",
      "2afef274a7ef44118f489e30f5dcd807",
      "074f53b738834d21952c4ef0e07f3eef",
      "5fe55f3d3e964960840c45d334e7eb4b",
      "2ea0682147024a8f815097dba915ffc8",
      "52b6e9dac665486293f81d16e975a925",
      "89159004831f4eafae8e6ade51fdbe04"
     ]
    },
    "id": "CojoywaliMEu",
    "outputId": "a5d9f231-befd-414e-fcc7-0b75f12abc5d"
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bp1wc1XmfQ-P"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7491bb4caa474da899c5a60226daf842",
      "38a43f210d0f4d1095cb2a22cc7395c3",
      "5373673963134ab38b9601b56eb3a720",
      "1615161435e34f7ebb256c17db3a6105",
      "a648fb809c60474eb838856f16f1f1ee",
      "fa8c8949c8354bb2992816aa93e4c22e",
      "3ae922bd962249e3b268fe32feb6cf70",
      "b76366a158004afb8dd7a1954354f322",
      "4525e0fd37cc4f7b846fbbc038d4929b",
      "b96f7019e9824104a75ff09ade1b691a",
      "b4efc4aa13ff437e83ec6f25eb5419ba",
      "48fb52121ac1419d84787c2082dc8deb",
      "fc80fad7056a4bb9b54c91b25c236b28",
      "dd9f4c7abc04446f98e6f93331d3cefe",
      "4ecc82399c914b4eb84fd8e7c2214452",
      "abe2e501b1c64fa08b9fd6bf70671841",
      "4f145d80eda943728a1dc7604877d74f",
      "26ebf233b452477d884a49ae173e43e0",
      "67006ccc3fd94b988b92d05cfa0f00d1",
      "bb2eca68e6184be5aa0b0ded561ec304",
      "10d1265b284b480a89657358c842439f",
      "183a6f656f164c4ba82dac758068003b",
      "61933304b557494e9315bf27aeb57242",
      "50c9be669aac4adcab85905da4c1c033",
      "85ccb9a5032b4217a5088a6666fac5a9",
      "6c991476f9b64fcda7a86774a429d3e1",
      "cb4a90df0cf147d79be5dface6297f49",
      "259b6959fa1d468eb16a2cbbcacbc9bb",
      "842738f1c3ee43fb8b8c5a8742d7252a",
      "9b277f5151f54a6c946d12325734cac8",
      "d58f89d521f143a194c3448075cb234c",
      "2331b5de9f99467ba5a3bf1fb6cb6cbe",
      "cd459536d31b429987c1625bded2f579"
     ]
    },
    "id": "f8SF269aKEli",
    "outputId": "5d3a558d-9c76-4863-842b-a0c088eb24ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 16:47:43.337241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 16:47:44.381857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"asr_skripsi_local_common_voice\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"asr_skripsi_local_common_voice/checkpoint-2400\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processorLM = Wav2Vec2ProcessorWithLM.from_pretrained(\"asr_LM_skripsi_local_common_voice\", eos_token=None, bos_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZkSCDmULT2U",
    "outputId": "50386f39-e884-49d7-c3de-7f0bdd31785a"
   },
   "outputs": [],
   "source": [
    "input_dict = processor(data_test_cleaned[7][\"audio_resampled\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "transcript = data_test_cleaned[7][\"sentence\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptionLM = processorLM.batch_decode(logits.cpu().detach().numpy()).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ImFWFPFNJj4",
    "outputId": "efa59758-3f18-411f-b74c-3d2f5ba0f44e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "jalanan sepi dari kendaraan\n",
      "\n",
      "Prediction LM:\n",
      "jalanan sepi dari kendaraan\n",
      "\n",
      "Reference:\n",
      "jalanan sepi dari kendaraan\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction:\")\n",
    "print(processor.decode(pred_ids))\n",
    "\n",
    "print(\"\\nPrediction LM:\")\n",
    "print(transcriptionLM[0])\n",
    "\n",
    "print(\"\\nReference:\")\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alckylzer/miniconda3/envs/AlcENV/lib/python3.10/site-packages/dill/_dill.py:1890: PicklingWarning: Pickling a PyCapsule (None) does not pickle any C data structures and could cause segmentation faults or other memory errors when unpickling.\n",
      "  warnings.warn('Pickling a PyCapsule (%s) does not pickle any C data structures and could cause segmentation faults or other memory errors when unpickling.' % (name,), PicklingWarning)\n",
      "Parameter 'function'=<function evaluate at 0x7f1a72225a20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee461b53faec4433bf7ef03009f42aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3618 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(batch):\n",
    "\n",
    "    input_dict = processor(batch[\"audio_resampled\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "\n",
    "    logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "    decoded = processor.decode(pred_ids)\n",
    "    batch[\"pred_strings\"] = decoded\n",
    "    return batch\n",
    "\n",
    "result = data_test_cleaned.map(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4197/3732520207.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer = load_metric(\"wer\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER Without LM : 12.940301\n"
     ]
    }
   ],
   "source": [
    "wer = load_metric(\"wer\")\n",
    "print(\"WER Without LM : {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alckylzer/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/id/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0/cache-061ad0d2021ac2cb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER With LM : 6.016472\n"
     ]
    }
   ],
   "source": [
    "def evaluateLM(batch):\n",
    "\n",
    "    input_dict = processor(batch[\"audio_resampled\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "\n",
    "    logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "    transcriptionLM = processorLM.batch_decode(logits.cpu().detach().numpy()).text[0]\n",
    "    \n",
    "    batch[\"pred_strings\"] = transcriptionLM\n",
    "    return batch\n",
    "\n",
    "resultLM = data_test_cleaned.map(evaluateLM)\n",
    "\n",
    "wer = load_metric(\"wer\")\n",
    "print(\"WER With LM : {:2f}\".format(100 * wer.compute(predictions=resultLM[\"pred_strings\"], references=result[\"sentence\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04b09958ad9049bba1a5ae95c215c132": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a3f04abf0a114579a97970a6a413be35",
        "IPY_MODEL_94747e94bb29455487f137e06132b1e7",
        "IPY_MODEL_458a2c191a3c46d79b1480cf0c44c498"
       ],
       "layout": "IPY_MODEL_4ea293a7676f4d818a42ad67eb16d6bd"
      }
     },
     "0b8b264617c5443d94bd8a5505a66302": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5e39326bbb6c486cbe819fb187534e2d",
        "IPY_MODEL_f70838f789854b84b5ac076426c84c91",
        "IPY_MODEL_d7ce36c8077c47c0a4484712ccfa70f9",
        "IPY_MODEL_4089df5889a045f0ac7fd3edf7a72d2c",
        "IPY_MODEL_5a8c3b1489054109b853d4b7cff9fd5d"
       ],
       "layout": "IPY_MODEL_5bf6082f85704fa189e52786b6d62095"
      }
     },
     "237672084fc24feeab1045c5e1dd5242": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "303f536dc5db47769756b5837724122f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37469569f09e4a0ea1399f6f31ff5119": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "383ac151389a415c9d6260f4314f472c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "38bf631531fa4b978c513c2b281bc939": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4089df5889a045f0ac7fd3edf7a72d2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Login",
       "layout": "IPY_MODEL_303f536dc5db47769756b5837724122f",
       "style": "IPY_MODEL_383ac151389a415c9d6260f4314f472c",
       "tooltip": null
      }
     },
     "450eb90f3b2e4d84976cefdd34bc9635": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "458a2c191a3c46d79b1480cf0c44c498": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5d9205950b714098a41a4897589cb72f",
       "style": "IPY_MODEL_887608a9cb8649a8875bcbfcff29ec0a",
       "value": " 5/5 [00:00&lt;00:00, 91.42it/s]"
      }
     },
     "4601b64a76cf4fe8b0bdcbdac8df3088": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "485c8633f55349f19db0127355255c6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4b197cc9619642d6a805b57c69e5e47e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4ea293a7676f4d818a42ad67eb16d6bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4ed501c18a23436e9157b96a9e9027f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5a8c3b1489054109b853d4b7cff9fd5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_69e7c682f1a54ac8bcb408faf41a0a3d",
       "style": "IPY_MODEL_750130d18dbd488685dc9e59e4c888b1",
       "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
      }
     },
     "5bf6082f85704fa189e52786b6d62095": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "align_items": "center",
       "display": "flex",
       "flex_flow": "column",
       "width": "50%"
      }
     },
     "5d9205950b714098a41a4897589cb72f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5e39326bbb6c486cbe819fb187534e2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dc910f6251ab49adbff745dcf6287d6b",
       "style": "IPY_MODEL_450eb90f3b2e4d84976cefdd34bc9635",
       "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
      }
     },
     "69e7c682f1a54ac8bcb408faf41a0a3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "750130d18dbd488685dc9e59e4c888b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "887608a9cb8649a8875bcbfcff29ec0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "94747e94bb29455487f137e06132b1e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_37469569f09e4a0ea1399f6f31ff5119",
       "max": 5,
       "style": "IPY_MODEL_237672084fc24feeab1045c5e1dd5242",
       "value": 5
      }
     },
     "9e5c8e06ec8444b097bae17b05af59e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a3f04abf0a114579a97970a6a413be35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4b197cc9619642d6a805b57c69e5e47e",
       "style": "IPY_MODEL_9e5c8e06ec8444b097bae17b05af59e5",
       "value": "100%"
      }
     },
     "d7ce36c8077c47c0a4484712ccfa70f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxModel",
      "state": {
       "description": "Add token as git credential?",
       "disabled": false,
       "layout": "IPY_MODEL_4601b64a76cf4fe8b0bdcbdac8df3088",
       "style": "IPY_MODEL_38bf631531fa4b978c513c2b281bc939",
       "value": true
      }
     },
     "dc910f6251ab49adbff745dcf6287d6b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f70838f789854b84b5ac076426c84c91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "PasswordModel",
      "state": {
       "description": "Token:",
       "layout": "IPY_MODEL_4ed501c18a23436e9157b96a9e9027f5",
       "style": "IPY_MODEL_485c8633f55349f19db0127355255c6c"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
